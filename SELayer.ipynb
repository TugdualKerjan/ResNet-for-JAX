{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll implement the SELayer, which tries to extract global features. Explained in the link below:\n",
    "\n",
    "https://amaarora.github.io/fastexplain/2020/07/24/SeNet.html\n",
    "\n",
    "\n",
    "\n",
    "    We expect the learning of convolutional features to be enhanced by explicitly modelling channel interdependencies, so that the network is able to increase its sensitivity to informative features which can be exploited by subsequent transformations. Consequently, we would like to provide it with access to global information and recalibrate filter responses in two steps, squeeze and excitation, before they are fed into the next transformation.\n",
    "\n",
    " ![Image showing how the Squeeze and Excite adds information](assets/se.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also using eqx.Module. All this seems to do is register the class as a PyTree for JAX to work with. PyTrees are JAX's idea to manage complex composed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take the global average.\n",
    "\n",
    "$$z_c = \\mathbf{F}_{sq}(\\mathbf{u}_c) = \\frac{1}{H \\times W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} u_c(i,j)$$\n",
    "\n",
    "Then apply FFN, ReLU, FFN and Sigmoid. Unsure why you would need both though ?\n",
    "\n",
    "$$\\mathbf{s} = \\mathbf{F}_{ex}(\\mathbf{z}, \\mathbf{W}) = \\sigma(g(\\mathbf{z}, \\mathbf{W})) = \\sigma(\\mathbf{W}_2 \\delta(\\mathbf{W}_1 \\mathbf{z}))$$\n",
    "\n",
    "The two FFN form bottleneck architecture.\n",
    "\n",
    "Finally, we multiply $$\\mathbf{s}\\mathbf{u_c}$$ to get the weighted channels.\n",
    "\n",
    "Why this excitation function ? To use the information of the sqeeze, follow it by capturing channel-wise dependencies. To do this excitation needs to: Be flexible, and learn non-mutually-exclusive relationships to make sure mutliple channels are emphasised.\n",
    "\n",
    "The role of the first **W_1** is to reduce the dimensionality.\n",
    "\n",
    "__This__ is why the activation is sigmoid, which allows multiple channels to have high importance compared to softmax which would impose importance on just one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to play around with:\n",
    "\n",
    "- [ ] Do we really need to reduce to one global average directly ?\n",
    "- [ ] Why use ReLU ? Maybe other solutions ?\n",
    "- [ ] What's the difference between AdaptiveAvgPool2d and AvgPool2d ? \n",
    "- [ ] Why not use 3d convolutions to understand dependencies between channels ? What about a conformer ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "\n",
    "\n",
    "class SELayer(eqx.Module):\n",
    "    fc1: eqx.nn.Linear\n",
    "    fc2: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, channel, key, reduction=8):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.fc1 = eqx.nn.Linear(channel, channel // reduction, use_bias=True, key=key1)\n",
    "        self.fc2 = eqx.nn.Linear(channel // reduction, channel, use_bias=True, key=key2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = eqx.nn.AdaptiveAvgPool2d(1)(x)\n",
    "        y = jax.numpy.squeeze(y)\n",
    "        y = self.fc1(y)\n",
    "        y = jax.nn.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = jax.nn.sigmoid(y)\n",
    "        y = jax.numpy.expand_dims(y, (1, 2))\n",
    "\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out the above block by feeding random values in a batch like manner. We use vmap function of JAX to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax._src.interpreters.batching.BatchTracer'>\n",
      "<class 'jax._src.interpreters.batching.BatchTracer'>\n",
      "(100, 256, 256)\n",
      "(100, 1, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(100, 100), (100, 100, 256, 256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/util.py:302\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrace_context_in_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_ignore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/util.py:296\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/lax/lax.py:161\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/lax/lax.py:177\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(100, 100), (100, 100, 256, 256)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SELayer(\u001b[38;5;241m100\u001b[39m, model_key, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Perform gradient descent\u001b[39;00m\n\u001b[1;32m     18\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# @jax.jit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(model, x, y):\n\u001b[1;32m      4\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(model)(x)\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnumpy\u001b[38;5;241m.\u001b[39mmean((\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred_y\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:1255\u001b[0m, in \u001b[0;36msubtract\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;129m@implements\u001b[39m(np\u001b[38;5;241m.\u001b[39msubtract, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubtract\u001b[39m(x: ArrayLike, y: ArrayLike, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m-> 1255\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;241m*\u001b[39m\u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubtract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/numpy/util.py:357\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    355\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    356\u001b[0m check_for_prngkeys(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/numpy/util.py:227\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    226\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 227\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39mbroadcast_to_rank(arg, result_rank) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jaxtts/lib/python3.11/site-packages/jax/_src/lax/lax.py:177\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    175\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(100, 100), (100, 100, 256, 256)]"
     ]
    }
   ],
   "source": [
    "@jax.grad\n",
    "# @jax.jit\n",
    "def loss(model, x, y):\n",
    "    pred_y = jax.vmap(model)(x)\n",
    "    return jax.numpy.mean((y - pred_y) ** 2)  # L2 Loss\n",
    "\n",
    "\n",
    "# loss = jax.grad(loss)\n",
    "\n",
    "x_key, y_key, model_key = jax.random.split(jax.random.PRNGKey(0), 3)\n",
    "# Example data\n",
    "x = jax.random.normal(x_key, (100, 100, 256, 256)).astype(jax.numpy.float32)\n",
    "y = jax.random.normal(y_key, (100, 100)).astype(jax.numpy.float32)\n",
    "\n",
    "model = SELayer(100, model_key, reduction=2)\n",
    "# Compute gradients\n",
    "grads = loss(model, x, y)\n",
    "# Perform gradient descent\n",
    "learning_rate = 0.1\n",
    "new_model = jax.tree_util.tree_map(lambda m, g: m - learning_rate * g, model, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to PyTorch\n",
    "\n",
    "We can also write the funcion in PyTorch and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define PyTorch version of SELayer\n",
    "class SELayerTorch(nn.Module):\n",
    "    def __init__(self, channel, reduction=8):\n",
    "        super(SELayerTorch, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32,)\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "torch.Size([1, 32, 64, 64])\n",
      "False\n",
      "Indices where the outputs differ:\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  1]\n",
      " [ 0  0  0  0  2]\n",
      " ...\n",
      " [ 0  0 31 63 61]\n",
      " [ 0  0 31 63 62]\n",
      " [ 0  0 31 63 63]]\n",
      "\n",
      "Differences at those points:\n",
      "[0.03054592 0.01835971 0.00503328 ... 0.18904695 0.09917451 0.20863083]\n"
     ]
    }
   ],
   "source": [
    "torch_model = SELayerTorch(32, reduction=8)\n",
    "torch_model.eval()\n",
    "\n",
    "# Initialize JAX model with same parameters\n",
    "key = jax.random.PRNGKey(0)\n",
    "jax_model = SELayerJax(32, key=key, reduction=8)\n",
    "\n",
    "# Example input for JAX model\n",
    "input_tensor_jax = jax.random.normal(key, (1, 32, 64, 64))\n",
    "output_jax = jax.vmap(jax_model)(input_tensor_jax)\n",
    "\n",
    "# Example input for PyTorch model\n",
    "print(type(input_tensor_jax))\n",
    "input_tensor_torch = torch.tensor(np.array(input_tensor_jax))\n",
    "print(input_tensor_torch.size())\n",
    "output_torch = torch_model(input_tensor_torch).detach().numpy()\n",
    "\n",
    "\n",
    "# Copy weights and biases from JAX model to PyTorch model\n",
    "with torch.no_grad():\n",
    "    torch_model.fc[0].weight.copy_(torch.tensor(np.array(jax_model.fc1.weight)))\n",
    "    torch_model.fc[0].bias.copy_(torch.tensor(np.array(jax_model.fc1.bias)))\n",
    "    torch_model.fc[2].weight.copy_(torch.tensor(np.array(jax_model.fc2.weight)))\n",
    "    torch_model.fc[2].bias.copy_(torch.tensor(np.array(jax_model.fc2.bias)))\n",
    "\n",
    "# Convert JAX output to numpy\n",
    "output_jax_np = np.array(output_jax)\n",
    "\n",
    "# Compare outputs\n",
    "print(\n",
    "    np.allclose(output_torch, output_jax_np, atol=1e-5)\n",
    ")  # Should return True if they match closely\n",
    "\n",
    "# Compute absolute differences\n",
    "differences = np.abs(output_torch - output_jax_np)\n",
    "\n",
    "# Find where the differences exceed the tolerance\n",
    "diff_exceeds_tolerance = differences > 1e-5\n",
    "\n",
    "# Print the indices and values of the elements that are different\n",
    "print(\"Indices where the outputs differ:\")\n",
    "print(np.argwhere(diff_exceeds_tolerance))\n",
    "\n",
    "\n",
    "print(\"\\nDifferences at those points:\")\n",
    "print(differences[diff_exceeds_tolerance])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
